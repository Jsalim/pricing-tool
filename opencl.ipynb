{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "import numpy as np\n",
    "import pyopencl as cl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The pyopencl.ipython_ext extension is already loaded. To reload it, use:\n",
      "  %reload_ext pyopencl.ipython_ext\n"
     ]
    }
   ],
   "source": [
    "%load_ext pyopencl.ipython_ext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choose platform:\n",
      "[0] <pyopencl.Platform 'Apple' at 0x7fff0000>\n",
      "Choice [0]:0\n",
      "Choose device(s):\n",
      "[0] <pyopencl.Device 'Intel(R) Core(TM) i7-4870HQ CPU @ 2.50GHz' on 'Apple' at 0xffffffff>\n",
      "[1] <pyopencl.Device 'Iris Pro' on 'Apple' at 0x1024500>\n",
      "[2] <pyopencl.Device 'AMD Radeon R9 M370X Compute Engine' on 'Apple' at 0x1021c00>\n",
      "Choice, comma-separated [0]:2\n",
      "Set the environment variable PYOPENCL_CTX='0:2' to avoid being asked again.\n"
     ]
    }
   ],
   "source": [
    "ctx = cl.create_some_context(interactive=True)\n",
    "queue = cl.CommandQueue(ctx, properties=cl.command_queue_properties.PROFILING_ENABLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device name: AMD Radeon R9 M370X Compute Engine\n",
      "Device type: GPU\n",
      "Device memory:  2048 MB\n",
      "Device max clock speed: 80 MHz\n",
      "Device compute units: 10\n"
     ]
    }
   ],
   "source": [
    "for device in ctx.get_info(cl.context_info.DEVICES):\n",
    "  print(\"Device name:\", device.name)\n",
    "  print(\"Device type:\", cl.device_type.to_string(device.type))\n",
    "  print(\"Device memory: \", device.global_mem_size//1024//1024, 'MB')\n",
    "  print(\"Device max clock speed:\", device.max_clock_frequency, 'MHz')\n",
    "  print(\"Device compute units:\", device.max_compute_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cl_kernel -o \"-cl-fast-relaxed-math\"\n",
    "\n",
    "__kernel\n",
    "void mmul(__global const float* a, __global const float* b, __global float* c){\n",
    "    int i = get_global_id(0); \n",
    "    c[i] = 0;\n",
    "    for(int j = 0; j < 300000; j++)\n",
    "    {\n",
    "        c[i] += a[i * 100 + j] * b[j]; \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cl_kernel -o \"-cl-fast-relaxed-math\"\n",
    "\n",
    "__kernel\n",
    "void mdot(__global const float* a, \n",
    "          __global const float* b, \n",
    "          __global float* c, \n",
    "          const int n, const int p, \n",
    "          __global int* offsets, \n",
    "          __local float* temp)\n",
    "{\n",
    "    int glid = get_global_id(0); \n",
    "    int pos = glid;\n",
    "    int offset = offsets[glid];\n",
    "    for(int i = 0; i < n; i++)\n",
    "    {\n",
    "        int idx = offset + a[pos]; \n",
    "        temp[idx] += b[i];\n",
    "        pos += p;\n",
    "    }\n",
    "    for(int i = 0; i < n; i++){\n",
    "        c[i] = temp[i];\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cl_kernel -o \"-cl-fast-relaxed-math\"\n",
    "\n",
    "__kernel\n",
    "void mdot(__global const uint8* a, __global const float* b, __global float* c, const int n){\n",
    "    int i = get_global_id(0); \n",
    "    float d = 0;\n",
    "    for(int j = 0; j < n; j++)\n",
    "    {\n",
    "        d += a[j * 100 + i] * b[j]; \n",
    "    }\n",
    "    c[i] = d;\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cl_kernel -o \"-cl-fast-relaxed-math\"\n",
    "\n",
    "__kernel \n",
    "void dot_product(__global float4* a_vec, \n",
    "                 __global float4* b_vec,\n",
    "                 __global float* output, \n",
    "                 __local float4* partial_dot) {\n",
    "\n",
    "   int gid = get_global_id(0);\n",
    "   int lid = get_local_id(0);\n",
    "   int group_size = get_local_size(0);\n",
    "\n",
    "   partial_dot[lid] = a_vec[gid] * b_vec[gid];\n",
    "   barrier(CLK_LOCAL_MEM_FENCE);\n",
    "\n",
    "   for(int i = group_size/2; i>0; i >>= 1) {\n",
    "      if(lid < i) {\n",
    "         partial_dot[lid] += partial_dot[lid + i];\n",
    "      }\n",
    "      barrier(CLK_LOCAL_MEM_FENCE);\n",
    "   }\n",
    "\n",
    "   if(lid == 0) {\n",
    "      output[get_group_id(0)] = dot(partial_dot[0], (float4)(1.0f));\n",
    "   }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 300000\n",
    "p = 1000\n",
    "A = np.random.rand(n * p).astype(np.float32).reshape((n, p))\n",
    "B = np.random.rand(n).astype(np.float32)\n",
    "C = np.zeros(p).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_buf = cl.Buffer(ctx, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=A)\n",
    "B_buf = cl.Buffer(ctx, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=B)\n",
    "C_buf = cl.Buffer(ctx, cl.mem_flags.WRITE_ONLY, C.nbytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mvmul.set_args(A_buf, B_buf, C_buf)\n",
    "mdot.set_args(A_buf, B_buf, C_buf, np.int32(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cl.get_platforms()[0].get_devices(cl.device_type.ALL)[2].max_work_group_size #max_work_item_dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Kernel Time: 0.11591096000000001 s\n",
      "GPU Time: 0.1342761516571045 s\n",
      "2.080762435746056 GFlops\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Axa/git/pricing-tool/venv/lib/python3.6/site-packages/ipykernel_launcher.py:5: DeprecationWarning: 'enqueue_read_buffer' has been deprecated in version 2011.1. Please use enqueue_copy() instead.\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "ev = cl.enqueue_nd_range_kernel(queue, mdot, (p,), None)\n",
    "ev.wait()\n",
    "gpu_start_time = time()  # Get the GPU start time\n",
    "ev = cl.enqueue_nd_range_kernel(queue, mdot, (p,), None)\n",
    "cl.enqueue_read_buffer(queue, C_buf, C)\n",
    "queue.finish()\n",
    "gpu_end_time = time()  # Get the GPU end time\n",
    "elapsed = 1e-9 * (ev.profile.end - ev.profile.start)  # Calculate the time it took to execute the kernel\n",
    "print(\"GPU Kernel Time: {0} s\".format(elapsed))  # Print the time it took to execute the kernel\n",
    "print(\"GPU Time: {0} s\".format(gpu_end_time - gpu_start_time))  # Print the time the GPU program took, including \n",
    "                                                                #both memory copies\n",
    "print((n * p)/(gpu_end_time - gpu_start_time) / (1024 * 1024 * 1024), \"GFlops\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([75008.414, 74864.51 , 75034.93 , 74927.42 , 75070.87 , 75055.29 ,\n",
       "       74818.57 , 75180.5  , 74918.96 , 74954.12 , 75095.9  , 74875.67 ,\n",
       "       74979.26 , 74972.89 , 74906.58 , 74910.5  , 75006.17 , 74995.   ,\n",
       "       74920.945, 74995.016, 74909.08 , 74821.73 , 74991.53 , 75054.28 ,\n",
       "       74977.77 , 74901.734, 74850.49 , 74906.57 , 74944.38 , 74984.195,\n",
       "       75021.71 , 74982.53 , 75006.44 , 74936.08 , 74938.66 , 74961.18 ,\n",
       "       74929.75 , 74911.71 , 74920.16 , 75039.01 , 75101.414, 75007.49 ,\n",
       "       74991.62 , 74937.16 , 74956.93 , 74985.68 , 75020.9  , 74995.336,\n",
       "       75040.266, 74997.875, 74966.18 , 74994.22 , 74981.06 , 74998.98 ,\n",
       "       75088.695, 74902.76 , 74941.055, 74907.25 , 75145.484, 74923.29 ,\n",
       "       75079.555, 74975.13 , 75222.99 , 74995.336, 75028.805, 75004.5  ,\n",
       "       75170.4  , 75022.57 , 75097.266, 75164.19 , 74823.96 , 74938.27 ,\n",
       "       75104.27 , 74913.49 , 75021.91 , 74955.22 , 74937.56 , 74834.734,\n",
       "       75166.38 , 74942.695, 74881.17 , 75071.484, 75091.266, 75028.83 ,\n",
       "       74925.78 , 74914.25 , 75027.8  , 74960.836, 74975.54 , 74854.23 ,\n",
       "       74851.78 , 75025.12 , 74955.516, 75016.54 , 75047.516, 74992.13 ,\n",
       "       74875.4  , 75021.23 , 74890.88 , 74956.51 , 74965.125, 74880.766,\n",
       "       75086.625, 74927.42 , 75047.31 , 75031.58 , 74818.96 , 75206.87 ,\n",
       "       74976.69 , 75000.52 , 75073.43 , 74880.375, 74966.58 , 74931.445,\n",
       "       74841.88 , 74950.97 , 75139.375, 74930.62 , 74761.13 , 75000.51 ,\n",
       "       74842.51 , 74853.46 , 74990.67 , 75070.51 , 75112.19 , 74919.555,\n",
       "       74841.555, 74900.875, 74940.67 , 74947.16 , 75038.03 , 74969.97 ,\n",
       "       74923.05 , 74976.97 , 75083.1  , 75020.44 , 74915.305, 74858.52 ,\n",
       "       74841.49 , 75012.28 , 75024.96 , 75086.32 , 74955.375, 74941.734,\n",
       "       74870.125, 74947.45 , 74997.88 , 74864.62 , 75103.   , 74913.52 ,\n",
       "       74960.46 , 74992.01 , 75051.92 , 75064.38 , 75036.164, 74963.27 ,\n",
       "       75058.54 , 74898.65 , 75210.41 , 74856.875, 75002.76 , 74962.14 ,\n",
       "       75267.8  , 74969.7  , 74970.43 , 75062.08 , 75133.14 , 74988.53 ,\n",
       "       75053.234, 75168.484, 74972.37 , 74925.45 , 75018.625, 75020.71 ,\n",
       "       74966.055, 74898.07 , 74946.68 , 74947.72 , 75134.695, 75044.016,\n",
       "       74890.54 , 75060.83 , 75082.26 , 75043.24 , 75017.2  , 74922.06 ,\n",
       "       74988.375, 74948.305, 74993.89 , 74829.62 , 74906.195, 75020.266,\n",
       "       74990.914, 74959.81 , 75049.875, 75001.77 , 74857.17 , 74931.86 ,\n",
       "       74843.516, 74963.58 , 74923.07 , 74873.76 , 75045.68 , 74948.59 ,\n",
       "       75116.26 , 75050.39 , 74809.56 , 75233.14 , 74944.766, 75016.27 ,\n",
       "       75033.05 , 75001.664, 74963.36 , 74902.06 , 74921.664, 74961.445,\n",
       "       75039.766, 75027.83 , 74815.63 , 75050.44 , 74984.23 , 74805.336,\n",
       "       74976.87 , 75006.05 , 75002.22 , 74862.555, 74772.84 , 74862.2  ,\n",
       "       74924.375, 74928.   , 74917.734, 75010.67 , 74894.07 , 74948.945,\n",
       "       75041.67 , 75006.88 , 74875.59 , 74874.47 , 74841.4  , 75133.76 ,\n",
       "       75002.32 , 75004.01 , 75018.21 , 74907.08 , 74959.77 , 75048.664,\n",
       "       75086.29 , 74906.01 , 75066.195, 74903.84 , 74916.46 , 74963.914,\n",
       "       75024.81 , 74920.484, 75096.375, 74940.266, 75079.62 , 74954.71 ,\n",
       "       75140.93 , 74982.234, 75039.89 , 74947.22 , 75158.695, 74996.59 ,\n",
       "       74977.24 , 75127.35 , 75062.11 , 74999.05 , 75122.086, 75144.445,\n",
       "       74893.04 , 74958.51 , 74956.08 , 75030.836, 75011.76 , 74959.37 ,\n",
       "       74917.77 , 74971.43 , 75152.82 , 75099.42 , 75012.96 , 75132.35 ,\n",
       "       75086.984, 74960.51 , 75047.8  , 75033.266, 74985.164, 74982.07 ,\n",
       "       75017.85 , 74768.62 , 74911.82 , 74956.42 , 75050.25 , 75024.63 ,\n",
       "       75076.16 , 74947.336, 74857.89 , 74940.33 , 74929.1  , 74996.41 ,\n",
       "       74953.86 , 74851.81 , 74957.88 , 74903.14 , 74999.664, 75030.664,\n",
       "       74844.79 , 75149.03 , 74886.31 , 74976.805, 75122.92 , 74914.36 ,\n",
       "       74962.59 , 74924.43 , 74874.33 , 74946.266, 75010.36 , 74972.35 ,\n",
       "       74900.32 , 75061.27 , 74846.26 , 74893.234, 75040.28 , 75056.83 ,\n",
       "       75046.57 , 74808.42 , 74824.82 , 74860.56 , 74877.914, 74961.71 ,\n",
       "       74936.086, 74949.36 , 74846.46 , 75022.32 , 74912.66 , 75001.9  ,\n",
       "       74799.07 , 74921.195, 74851.914, 75099.695, 75082.82 , 75018.28 ,\n",
       "       74927.484, 74969.8  , 74903.58 , 74985.586, 75115.69 , 74884.51 ,\n",
       "       75015.13 , 74935.016, 74913.016, 75055.35 , 75047.15 , 74916.99 ,\n",
       "       75030.99 , 74895.99 , 74979.   , 74901.805, 75203.43 , 75012.91 ,\n",
       "       75057.836, 75011.54 , 75191.17 , 74942.87 , 74987.84 , 75089.47 ,\n",
       "       75099.41 , 74916.45 , 75043.92 , 75124.05 , 74813.32 , 75046.586,\n",
       "       75006.586, 74948.98 , 74932.625, 74886.55 , 74965.484, 74941.88 ,\n",
       "       75145.35 , 75012.766, 74864.516, 75146.305, 75032.95 , 75012.664,\n",
       "       75001.31 , 74878.65 , 74971.61 , 74951.67 , 74906.805, 74762.25 ,\n",
       "       74953.83 , 74976.125, 74939.98 , 75043.195, 74907.89 , 75035.33 ,\n",
       "       74968.055, 74948.77 , 74990.35 , 74916.24 , 74928.94 , 74889.72 ,\n",
       "       75031.664, 74965.69 , 75120.766, 75024.78 , 74796.6  , 75102.83 ,\n",
       "       74953.74 , 74974.375, 75085.78 , 74906.484, 74943.195, 74897.01 ,\n",
       "       74922.28 , 74912.76 , 75092.29 , 74954.67 , 74865.586, 75011.305,\n",
       "       74910.45 , 74871.055, 74941.   , 75013.586, 74995.63 , 74857.086,\n",
       "       74931.27 , 74898.945, 75052.89 , 75041.56 , 74967.64 , 74962.23 ,\n",
       "       74954.4  , 75015.28 , 75007.09 , 75013.35 , 74924.   , 74890.984,\n",
       "       74826.81 , 75014.5  , 75019.914, 75014.26 , 74978.336, 74880.33 ,\n",
       "       74858.08 , 74917.4  , 75091.52 , 74889.84 , 75116.35 , 74985.64 ,\n",
       "       74993.945, 75009.26 , 75016.6  , 74998.17 , 75086.95 , 74910.16 ,\n",
       "       75006.51 , 74861.84 , 75224.375, 74871.11 , 75120.96 , 75006.35 ,\n",
       "       75213.29 , 75083.01 , 75001.39 , 74989.43 , 75096.21 , 74979.27 ,\n",
       "       75043.18 , 75156.83 , 74856.63 , 74912.43 , 74996.875, 74924.68 ,\n",
       "       75002.48 , 74906.086, 74910.34 , 74850.64 , 75212.164, 74990.24 ,\n",
       "       74989.016, 75092.484, 75138.695, 75013.18 , 75023.74 , 75021.484,\n",
       "       75087.6  , 74972.16 , 74924.68 , 74824.25 , 75006.98 , 74932.75 ,\n",
       "       74855.88 , 75077.58 , 75004.57 , 74950.49 , 74941.336, 74938.83 ,\n",
       "       75004.54 , 74977.53 , 74919.18 , 74915.15 , 74972.73 , 74967.59 ,\n",
       "       75073.99 , 75036.38 , 74812.805, 75103.34 , 74881.77 , 74958.34 ,\n",
       "       75026.94 , 74894.88 , 74973.48 , 74901.766, 74903.29 , 74984.57 ,\n",
       "       75117.85 , 74891.29 , 74796.87 , 74952.49 , 74906.266, 74862.43 ,\n",
       "       74964.42 , 75079.484, 75049.11 , 74924.06 , 74849.68 , 74903.58 ,\n",
       "       74920.76 , 74938.33 , 74971.63 , 74986.43 , 74954.96 , 75017.484,\n",
       "       74961.59 , 75047.19 , 74862.63 , 74868.305, 74874.67 , 74975.23 ,\n",
       "       74915.28 , 75046.78 , 74911.44 , 74926.46 , 74943.92 , 74964.22 ,\n",
       "       75033.73 , 74911.25 , 75029.664, 75031.91 , 74916.27 , 75031.55 ,\n",
       "       75077.78 , 74960.94 , 75109.78 , 75013.45 , 74990.44 , 74940.25 ,\n",
       "       75145.98 , 74923.64 , 75032.87 , 74914.47 , 75208.81 , 74908.086,\n",
       "       74998.5  , 75061.86 , 75098.39 , 74918.14 , 75066.805, 75189.75 ,\n",
       "       74795.71 , 74900.14 , 75110.516, 74924.51 , 75080.93 , 74942.86 ,\n",
       "       75069.555, 74825.195, 75154.945, 74968.27 , 74879.766, 75087.7  ,\n",
       "       75099.63 , 75043.98 , 75047.   , 74975.266, 75051.13 , 75032.555,\n",
       "       74976.55 , 74819.15 , 74906.836, 74948.3  , 74970.14 , 75003.68 ,\n",
       "       75057.234, 75041.48 , 74990.625, 74983.06 , 74916.28 , 74986.63 ,\n",
       "       75019.2  , 74939.625, 74969.93 , 74871.52 , 75069.7  , 75019.56 ,\n",
       "       74790.43 , 75147.98 , 74975.586, 75078.11 , 75107.32 , 74916.19 ,\n",
       "       75005.06 , 74951.06 , 74929.44 , 75070.2  , 75080.32 , 74987.5  ,\n",
       "       74868.72 , 75015.695, 74856.664, 74966.83 , 75022.31 , 75012.   ,\n",
       "       75065.85 , 74949.125, 74863.53 , 74900.9  , 75045.26 , 74999.36 ,\n",
       "       75005.19 , 74987.58 , 74857.03 , 74918.72 , 75001.95 , 75002.63 ,\n",
       "       74947.266, 74969.266, 74895.21 , 75056.17 , 75043.33 , 75073.64 ,\n",
       "       74967.28 , 74963.734, 74979.13 , 74961.27 , 75076.41 , 74936.84 ,\n",
       "       75104.266, 75005.86 , 74987.59 , 75029.28 , 74996.28 , 74863.   ,\n",
       "       74964.1  , 75021.11 , 75074.49 , 74841.03 , 75207.15 , 74858.77 ,\n",
       "       75137.86 , 74933.94 , 75159.984, 75003.16 , 74925.85 , 74947.2  ,\n",
       "       75118.42 , 74964.62 , 75039.42 , 75178.34 , 74869.8  , 74902.586,\n",
       "       74996.03 , 74988.56 , 74983.77 , 74836.93 , 74882.43 , 74925.41 ,\n",
       "       75142.57 , 75134.92 , 74905.63 , 75155.87 , 75062.07 , 74955.734,\n",
       "       75077.69 , 74936.71 , 75094.35 , 75012.22 , 75014.34 , 74835.79 ,\n",
       "       75003.15 , 75029.18 , 74953.07 , 75006.17 , 74983.84 , 74963.96 ,\n",
       "       74878.84 , 74915.58 , 74950.125, 75109.875, 75058.45 , 74905.766,\n",
       "       75063.64 , 74906.984, 75013.58 , 75050.08 , 74816.99 , 75140.85 ,\n",
       "       74972.125, 74987.734, 75064.266, 74964.58 , 74961.36 , 74931.51 ,\n",
       "       74970.234, 74968.57 , 75053.85 , 74947.82 , 74841.66 , 75067.77 ,\n",
       "       74873.77 , 74895.35 , 74881.445, 74951.42 , 75035.28 , 74987.27 ,\n",
       "       74892.94 , 74912.58 , 74909.125, 75018.89 , 74846.95 , 74969.11 ,\n",
       "       74891.27 , 75015.91 , 74973.44 , 75028.33 , 74823.33 , 74840.19 ,\n",
       "       74847.82 , 75008.984, 74996.35 , 75027.984, 74948.99 , 74904.305,\n",
       "       74989.414, 74956.66 , 75071.1  , 74879.1  , 75081.016, 74939.89 ,\n",
       "       74863.64 , 75040.95 , 75050.6  , 74911.15 , 75106.37 , 74958.77 ,\n",
       "       74983.77 , 74884.5  , 75106.69 , 75006.34 , 75043.78 , 75029.01 ,\n",
       "       75187.234, 74944.98 , 75008.82 , 75022.81 , 75146.64 , 74939.21 ,\n",
       "       75112.34 , 75098.3  , 74897.2  , 74938.875, 75015.25 , 74931.05 ,\n",
       "       75109.78 , 74946.94 , 74926.08 , 74871.94 , 75128.914, 75003.71 ,\n",
       "       74856.945, 75059.1  , 74999.734, 74955.51 , 75056.62 , 74948.39 ,\n",
       "       75027.85 , 74997.86 , 74979.17 , 74863.93 , 74998.89 , 74908.414,\n",
       "       74962.805, 74969.97 , 75007.07 , 75074.07 , 74871.87 , 74941.86 ,\n",
       "       74927.14 , 75005.91 , 74975.   , 74963.26 , 75033.45 , 74974.06 ,\n",
       "       75000.39 , 75040.7  , 74739.24 , 75231.336, 74933.41 , 74967.695,\n",
       "       75086.19 , 74904.016, 75025.734, 74982.836, 74901.195, 74965.74 ,\n",
       "       75022.69 , 74928.64 , 74802.08 , 75034.1  , 74932.836, 74960.91 ,\n",
       "       74944.59 , 75009.62 , 75028.26 , 74861.7  , 74851.94 , 74838.195,\n",
       "       74904.42 , 74881.03 , 74970.98 , 75090.   , 74935.77 , 74976.4  ,\n",
       "       75048.25 , 75042.35 , 74855.63 , 74944.48 , 74931.49 , 75111.35 ,\n",
       "       75044.9  , 75006.05 , 75002.125, 74920.13 , 74981.95 , 74985.77 ,\n",
       "       75040.42 , 74899.414, 75027.95 , 74963.54 , 74928.61 , 75033.41 ,\n",
       "       75062.28 , 74976.234, 75053.516, 74934.82 , 75043.44 , 74896.305,\n",
       "       75210.77 , 74903.83 , 75041.24 , 75016.85 , 75203.234, 75016.07 ,\n",
       "       74991.82 , 74925.195, 75174.586, 74909.98 , 75063.25 , 75119.53 ,\n",
       "       74924.29 , 74952.89 , 75029.6  , 75022.4  , 75086.445, 74904.06 ,\n",
       "       74942.51 , 74908.99 , 75143.195, 74998.96 , 74832.46 , 75096.51 ,\n",
       "       75153.79 , 74960.24 , 74988.38 , 74933.01 , 74939.85 , 75130.664,\n",
       "       74902.64 , 74757.96 , 74904.25 , 74953.28 , 74942.55 , 75039.39 ,\n",
       "       75062.24 , 74979.305, 74911.51 , 74922.41 , 74900.66 , 75044.83 ,\n",
       "       74987.44 , 74937.84 , 75103.59 , 75016.24 , 75119.72 , 75023.375,\n",
       "       74886.84 , 75174.52 , 74945.04 , 75015.2  , 75003.03 , 74900.02 ,\n",
       "       75013.47 , 74900.15 , 74861.54 , 74937.805, 75057.984, 74936.55 ,\n",
       "       74851.31 , 75018.7  , 74944.78 , 74864.65 , 74984.09 , 75119.72 ,\n",
       "       75006.586, 74975.12 , 74817.61 , 74951.6  , 74911.6  , 74969.914,\n",
       "       75126.32 , 74971.86 , 74887.28 , 74994.125, 75010.32 , 74988.91 ,\n",
       "       74811.61 , 74911.34 , 74915.25 , 75044.26 , 74966.34 , 75002.16 ,\n",
       "       74984.67 , 74902.   , 74909.37 , 74959.72 , 75050.88 , 75035.01 ,\n",
       "       75171.82 , 74959.04 , 74958.56 , 75033.42 , 74990.52 , 74910.99 ,\n",
       "       75121.63 , 74926.414, 75054.66 , 74935.586, 75099.56 , 74930.81 ,\n",
       "       75036.   , 74844.6  , 75140.9  , 74940.664, 75012.28 , 74972.74 ,\n",
       "       75076.36 , 75006.92 , 75082.38 , 75137.06 , 74910.47 , 74938.91 ,\n",
       "       75067.07 , 74996.53 , 75041.414, 74837.53 , 74917.67 , 74826.445,\n",
       "       75133.68 , 75039.98 , 74836.516, 75039.555, 75156.28 , 74991.984,\n",
       "       75066.414, 74870.59 , 74984.99 , 75015.016, 74946.15 , 74788.56 ,\n",
       "       74972.48 , 74947.64 , 74920.734, 74956.945, 75040.125, 75027.805,\n",
       "       74959.99 , 75017.24 , 74861.11 , 75015.55 ], dtype=float32)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 s ± 59.3 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "(B[:, np.newaxis] * A).sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Time: 0.008141040802001953 s\n",
      "6.8639079247935335 GFlops\n"
     ]
    }
   ],
   "source": [
    "cpu_start_time = time()  # Get the GPU start time\n",
    "D = np.dot(A, B)\n",
    "cpu_end_time = time()  # Get the GPU end time\n",
    "print(\"CPU Time: {0} s\".format(cpu_end_time - cpu_start_time))  \n",
    "print((n * p * 2)/(cpu_end_time - cpu_start_time) / (1024 * 1024 * 1024), \"GFlops\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([24.567423, 22.538635, 23.54259 , ..., 25.078547, 25.717302,\n",
       "       26.456945], dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([24.567429, 22.538643, 23.542585, ..., 25.07854 , 25.717293,\n",
       "       26.456944], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(A, B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23217041877163042"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(cpu_end_time - cpu_start_time) / (gpu_end_time - gpu_start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([28.689331, 26.380375, 26.65357 , ..., 23.372551, 26.52175 ,\n",
       "       27.61783 ], dtype=float32)"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([28.689327, 26.380373, 26.653566, ..., 23.372551, 26.52174 ,\n",
       "       27.617823], dtype=float32)"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyclblast\n",
    "\n",
    "from pyopencl.array import Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Setting up Numpy arrays\n",
      "# Setting up OpenCL arrays\n"
     ]
    }
   ],
   "source": [
    "# Settings for this sample\n",
    "dtype = 'float32'\n",
    "alpha = 1\n",
    "beta = 0\n",
    "\n",
    "print(\"# Setting up Numpy arrays\")\n",
    "y = np.random.rand(n).astype(dtype=dtype)\n",
    "\n",
    "print(\"# Setting up OpenCL arrays\")\n",
    "cla = Array(queue, A.shape, A.dtype)\n",
    "clx = Array(queue, B.shape, B.dtype)\n",
    "cly = Array(queue, y.shape, y.dtype)\n",
    "cla.set(A)\n",
    "clx.set(x)\n",
    "cly.set(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Result for vector y: [27.239933 25.933067 31.403833 ... 24.658667 28.381613 26.393766]\n"
     ]
    }
   ],
   "source": [
    "pyclblast.gemv(queue, n, p, cla, clx, cly, a_ld=p, alpha=alpha, beta=beta)\n",
    "queue.finish()\n",
    "print(\"# Result for vector y: %s\" % cly.get())\n",
    "#print(\"# Expected result:     %s\" % (alpha * np.dot(A, B) + beta * y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([28.689327, 26.380373, 26.653566, ..., 23.372551, 26.52174 ,\n",
       "       27.617823], dtype=float32)"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(alpha * np.dot(A, B) + beta * y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
